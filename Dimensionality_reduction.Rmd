---
title: "PCA_calculations"
author: "Pranav Natarajan"
date: "06/05/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading Required Packages into workbook instance
```{r}
require(tidyr)# for easy data handling and pipeline creation
require(dplyr)# for easy data handling and pipeline creation
require(caret)# for machine learning models
require(Boruta)# for variable selection
require(xgboost)# for xgboost algorithms
#require(FactoMineR)# for FAMD
#require(factoextra)# for FAMD
#require(standardize)# for standardisation of continuous variables
```

## Loading comprehensive_stats from the rds file
```{r}
player_stats<- readRDS(file="player_stats.rds") # 9628 rows of 76 variables! OK
```

## getting summary of comprehensive stats
```{r}
summary(player_stats)
```

## Removing `SlugTeamsBREF` since `slugTeamBREF` already present.
```{r}
player_stats<- player_stats %>% select(!c(slugTeamsBREF)) #76 variables
```

## Removing further irrelevant columns before PCA computations
```{r}
player_stats<- player_stats %>% select(!c(yearSeason:groupPosition, # do not need personal identifiers
                                                        isHOFPlayer, # not required-- HOF attained only after retirement
                                                        idPlayerNBA, # not required
                                                        totSalary, salary_cap, # normalised salary already contains both their information
                                                        )) # 67 columns left
```


## getting revised summary of `player_stats`
```{r}
summary(player_stats)
```

## Removing `slugPlayerBREF` from feature set
```{r}
player_stats<- player_stats %>% select(!c(slugPlayerBREF))
```

## Creating the label set y = normalised_salary
```{r}
y = player_stats$normalised_salary
```

## Creating feature:- games not started as the difference between games and games started:- thereby reducing dependency on two columns in place of 1
```{r}
player_stats<- player_stats %>% mutate(countGamesNotStarted = countGames - countGamesStarted, .before="pctFG")
```

## Creating the dataframe of features
```{r}
X = player_stats %>% select(!c(normalised_salary))
```

## getting summary of X
```{r}
summary(X)
```

## Relocating the `slugTeamBREF` column to be after `slugPosition`
```{r}
X<- X %>% relocate(slugTeamBREF, .after = slugPosition)
```

## standardising the numeric columns
```{r}
# standard scaling the numeric columns
X[, -(1:2)]<- scale(x= X[, -(1:2)], center = T, scale=T)
```

## Variable Selection

### Step 1:- removing redundancies

We will remove the per minute stats as advanced stats already has on floor percentages or specialised transformations of the same.

From the above and reading the spec text files, we note that we already have 

1. eFG% <`pctEFG`> -- Effective Field Goal Percentage, a percentage statistic that adjusts for the fact that a 3-point field goal is worth one more point than a 2-point field goal. This is, in essence, a transformation of the per game percentages of field goals, 2 pt field goals, 3 pt field goals present in the dataset. So, we will delete `pctFG:pctFG2`, (and `fgmPerGame:ftaPerGame`, on which the percentages depend upon).

2. Free throw percentage need not be computed since Free throw percentage throughout season is given by Advanced stat `pctFTRate`. So, we will delete `pctFT`

3. Note that the Advanced stats has percentages for steals, blocks, turnovers, assists, rebounds (including Offensive Rebounds and Defensive Rebounds encompassed by Total Rebounds). So, we can delete `orbPerGame:tovPerGame`. We will retain Personal Fouls and Points as per game stats

4. We will focus on Total rebounds, which factor in offensive and defensive rebounds, so we can delete `pctORB:pctDRB`

5. Similar logic as in 4. for Box Plus Minus, and Win shares. So, we will delete `ratioOWS:ratioDWS` and `ratioWSPer48:ratioDBPM`

6. Remove all per minute stats as mentioned above. So, delete `fgmPerMinute:ptsPerMinute`.

Doing so,
```{r}
X<- X %>% select(!c(pctFG:pctFG2, fgmPerGame:ftaPerGame, 
                    pctFT, orbPerGame:tovPerGame, 
                    pctORB:pctDRB, ratioOWS:ratioDWS, ratioWSPer48:ratioDBPM,
                    fgmPerMinute:ptsPerMinute))
```

Revisiting the summary
```{r}
summary(X)
```

## Saving X and y for future use
```{r}
# saving X
saveRDS(object = X, file = "X.rds")
# saving y
saveRDS(object = y, file = "y.rds")
```

<!-- ## Stepwise AIC selection, forward and backward selection. Focus on per game stats. glance @ formulae for advanced stats and check for redundancies. check the MASS library. Variable importance function. Standardise the numeric functions before you do FAMD and see whether that makes a difference. -->

## The Boruta Algorithm for Variable Selection

Noting that we are not necessarily due to perform Linear lEast Squares regression, or any from a suite of usual glm/lm model objects, we will focus on the Boruta Algorithm, which uses a random forest regressor and the principle of shadow features

```{r}
Boruta_feat_selection<- Boruta(x = X, y = y, doTrace = 2, maxRuns = 250)
```

Getting the feature importance plots
```{r}
plot(Boruta_feat_selection, las=2, cex.axis=0.5)
```


```{r}
plotImpHistory(Boruta_feat_selection)
```

we see that these plots both do not have any of our 23 variables as tentative or rejected, as shown by random forest feature importance.

## Creating 80-20 training and testing set from X
```{r}
set.seed(42) # for reproducability

# getting training tuple indices
# and splitting based on quantiles of y
# similar to stratified splittin in which the 'distribution' 
# of the target is maintained in both training and test sets
train_indices<- createDataPartition(y = y, p = 0.8, list = F)

# creating training and testing sets
X_train<- X[train_indices,]
y_train<- y[train_indices]
X_test<- X[-(train_indices),]
y_test<- y[-(train_indices)]
```


## fitting and tuning a Random Forest Regressor's number of random samples chosen as well as the number of trees using 10 fold CV, and the evaluation metric as `RMSE`
```{r}
# instantiating the train control object
trControl<- trainControl(method = "cv", number=5, search = "grid", verboseIter = T)
# instantiating the grid
tuneGrid<- expand.grid(mtry=seq(1:23), splitrule=c("variance"), min.node.size=c(5,6,7))
# setting the seed to 42 for reproducibility
set.seed(42)
```

```{r}
# Running 5 fold CV grid Search of mtry values
# changed plans from repeated CV due to system inhibition
gridSearch_10FoldCV<- train(x = X_train,
                                y = y_train, 
                                method = "ranger",
                                metric = "RMSE",
                                trControl = trControl,
                                tuneGrid = tuneGrid)
```

## Getting the optimised hyperparameters
```{r}
gridSearch_10FoldCV$bestTune
```
We have 14 (independent) variables to possibly split at in each node.

## Getting the best fit model, already fit on the training set.
```{r}
nba_rfr<- gridSearch_10FoldCV$finalModel
```

## getting the OOB scores (i.e., the overall MSE of out of bag samples) of the model
```{r}
paste("OOB Score = MSE = ", nba_rfr$prediction.error)
paste("RMSE = ", sqrt(nba_rfr$prediction.error))
```

Note that the OOB Score is pretty good, while the RMSE is around 5.5% of the yearly salary cap.

## Getting the predictions on the test set:-
```{r}
nba_rfr_preds<- predict(object = nba_rfr, data = X_test)
```


## Getting the RMSE on the test set as a metric of performance
```{r}
rfr_RMSE<- RMSE(pred = nba_rfr_preds$predictions, obs = y_test)
paste("RMSE = ", rfr_RMSE)
```

Note that the test set error closely agrees with the out of bag error.

## Saving the model training object (with the best fit model) and RMSE Score
```{r}
saveRDS(object=nba_rfr, "nba_rfr.rds")
```

```{r}
saveRDS(object=rfr_RMSE, "nba_RMSE.rds")
```

Now, noting that in most cases, optimised tree based algorithms fit the training set better, possibly even overfit, we wish to compare this with a linear model.

First, we will compare an xgBoost Tree based regressor and an xgBoost Linear weighted function regressor. We will tune hyperparameters as well

## E(X)treme (G)radient Boosting ensemble methods.


Before we do CV, we will run the Boruta algorithm again, this time using xgBoost as the model object to derive variable importances from.

```{r}
Boruta_feat_selection_xgboost<- Boruta(x = X, y = y, doTrace = 2, maxRuns = 250, getImp = getImpXgboost)
```

Viewing the variable stats upon running 
```{r}
vars_for_selection<- attStats(Boruta_feat_selection_xgboost)
plot(Boruta_feat_selection_xgboost, las=2, cex.axis=0.5)
```

```{r}
plotImpHistory(Boruta_feat_selection_xgboost)
```
Note that there are only confirmed and rejected, no tentative variables. So, we can focus on the variables that are confirmed, and extract them as the training feature set for the xgBoost algorithms.

```{r}
# getting subset of confirmed variables
vars_for_selection_conf<- vars_for_selection %>% filter(decision == "Confirmed") %>% select(decision)
```

```{r}
# Getting the training Set for xgBoost
# note that the vector decision has rownames denoting each decision. So:
X_train_xgboost<- X_train %>% select(rownames(vars_for_selection_conf))
X_test_xgboost<- X_test %>% select(rownames(vars_for_selection_conf))
```

Now, we will run 5 fold Cross Validation on the grid parameters to find the best hyperparameters for the xgBoost algorithms.
```{r}
# instantiating the train control object
trControl<- trainControl(method = "cv", number=5, search = "grid", verboseIter = T)
# instantiating the grid for xbgTree
tuneGridTree<- expand.grid(nrounds=100, 
                           max_depth=c(5,6,7), 
                           eta=c(0.1, 0.3, 0.5), 
                           gamma=c(0, 0.05, 0.1, 0.5, 1), 
                           colsample_bytree = 1, 
                           min_child_weight =1, 
                           subsample=1)
# instantiating the grid for xgbLinear
tuneGridLinear<- expand.grid(nrounds=100, 
                             lambda=c(0, 0.05, 0.1, 0.5, 1), 
                             alpha=c(0, 0.05, 0.1, 0.5, 1),
                             eta=c(0.1, 0.3, 0.5))
# setting the seed to 42 for reproducibility
set.seed(42)
```

```{r}
# running the grid Search CV for the xgboost tree

set.seed(42) # for reproducibility

gridSearch_5FoldCV_Tree<- train(x=X_train_xgboost,
                                y=y_train,
                                method="xgbTree",
                                metric="RMSE",
                                trControl=trControl,
                                tuneGrid=tuneGridTree)
```

```{r}
# getting the optimised parameters for the xgbTree
gridSearch_5FoldCV_Tree$bestTune
```

```{r}
# saving the best model in a variable, and saving variable in an RDS file
nba_xgbTree<- gridSearch_5FoldCV_Tree$finalModel
saveRDS(object=nba_xgbTree, file="nba_xgbTree.rds")
```

```{r}
# running the grid Search CV for the xgboost linear

set.seed(42) # for reproducibility

gridSearch_5FoldCV_Linear<- train(x=X_train_xgboost,
                                  y=y_train,
                                  method="xgbLinear",
                                  metric="RMSE",
                                  trControl=trControl,
                                  tuneGrid=tuneGridLinear)
```

```{r}
# getting the optimised parameters for the xgbTree
gridSearch_5FoldCV_Linear$bestTune
```

```{r}
# saving the best model in a variable, and saving variable in an RDS file
nba_xgbLinear<- gridSearch_5FoldCV_Linear$finalModel
saveRDS(object=nba_xgbLinear, file="nba_xgbLinear.rds")
```


```{r}
# getting the predictions on the test set
nba_xgbTree_preds<- predict(object=nba_xgbTree, newdata=as.matrix(X_test_xgboost))
nba_xgbLinear_preds<- predict(object=nba_xgbLinear, newdata=as.matrix(X_test_xgboost))
```

```{r}
# getting RMSEs
nba_RMSE_xgbTree<- RMSE(pred=nba_xgbTree_preds, obs=y_test)
nba_RMSE_xgbLinear<- RMSE(pred=nba_xgbLinear_preds, obs=y_test)
```

```{r}
# saving RMSE values
saveRDS(object=nba_RMSE_xgbLinear, file="nba_RMSE_xgbLinear.rds")
saveRDS(object=nba_RMSE_xgbTree, file="nba_RMSE_xgbTree.rds")
```

```{r}
paste("xgbTree RMSE = ", nba_RMSE_xgbTree)
paste("xgbLinear RMSE = ", nba_RMSE_xgbLinear)
```

We see that both xgBoost models (and the combination of hyperparameters chosen) perform poorer out of sample than the Random Forest Model.

## TODO:- Run Elastic Net Regression, hyperparameter tuning of lambda and alpha (for l1 and l2 regularisation penalty terms)